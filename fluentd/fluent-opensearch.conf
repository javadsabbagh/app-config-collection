<source>
  @type           tail
  read_from_head  true
  ## Note: you can provide several file paths separated by comma.
  path            /opt/logs/*.log
  ## There is one position file and each file marks placed in the one line of following file
  pos_file        /opt/logs/positions/fluentd-log.pos
  tag             chq-apps-logs.*

  <parse>
    @type         json
    time_type     string
    #time_format   "%Y-%m-%dT%H:%M:%S.%N%z"    #  as in logs: ["@timestamp" : "2023-12-04T11:34:56.434+03:30"]
    time_key      "@timestamp"
    keep_time_key true
    #local_time    true
    #utc          true
    #timezone     "+03:30"
  </parse>
</source>


## Note: you can remove this filter and use record_modifier for simple use cases
<filter chq-apps-logs.**>
  @type record_transformer
  enable_ruby
  # remove unnecessary fields
  remove_keys ["thread_name", "level_value", "@version"]
</filter>


## Note: this plugin is faster than record_transformer plugin.
## But it has limited features and needs to be installed:
# [https://github.com/repeatedly/fluent-plugin-record-modifier]
## gem install fluent-plugin-record-modifier --no-document
<filter chq-apps-logs.**>
  @type record_modifier

  ## remove thread_name, level_value, and @version keys from record
  ## Note: fields X-Span-Export, X-B3-SpanId, X-B3-TraceId have corresponding spanExportable, spanId, traceId fields in logs
  ### Then removing theses duplicated fields
  remove_keys thread_name, level_value, @version, X-Span-Export, X-B3-SpanId, X-B3-TraceId
</filter>

<match chq-apps-logs.**>
    @type opensearch
    host 192.168.100.119
    scheme https
    port 9200
    # This option needs timekey property to be enabled in buffer section
    index_name chq.%Y-%m-%d
    user logger
    password gyinbEy@kE8L9ExG
    ssl_verify false

    ## for more info on buffer config please refer: [https://docs.fluentd.org/configuration/buffer-section]
    <buffer time, tag>
        @type memory
        flush_thread_count  2
        flush_mode          interval
        ## Enabling this config will stuck application when stopping fluentd
        #flush_at_shutdown   true
        flush_interval      5s
        chunk_limit_size    5m
        chunk_limit_records 500

        # If you want to chunk indices based on time you need to activate this feature with the following property.
        timekey 1h # chunks per hours ("3600" also available)
    </buffer>
</match>


### For debugging purpose you can send log to output
<match chq-apps-logs.**>
  @type stdout
  @id stdout_output

  ## For more info on output formatting: [https://docs.fluentd.org/configuration/format-section#formatter-plugin-type]
  <format>
    output_type  json
  </format>
</match>
